{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Content-based (Usando modelo BERT y sentence transformers)\n",
        "\n",
        "\n",
        "En este proyecto trabajaremos con un modelo de recomendacion de libros de la página [Goodreads](http://www.goodreads.com). El modelo de recomendación de libros es un recomendador basado en contenido, donde se utilizan modelos de lenguage BERT y sentence transformers para el cálculo de embeddings de los libros y luego similaridades de ítems. Luego, dependiendo de los libros con los que el usuario ha interactuado, se recomiendan los ítems más similares."
      ],
      "metadata": {
        "id": "RuyHWtoRWyDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import requests\n",
        "import heapq\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from io import BytesIO\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import time"
      ],
      "metadata": {
        "id": "UmN_Lx2KWxoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-cache --backups=1 \"https://raw.githubusercontent.com/valegrajales/laboratorio4-sr-uniandes/main/data/book_data.parquet.gzip\" -O book_data.parquet.gzip\n",
        "!wget --no-cache --backups=1 \"https://raw.githubusercontent.com/valegrajales/laboratorio4-sr-uniandes/main/data/goodreads_interactions.csv\" -O goodreads_interactions.csv\n",
        "!wget --no-cache --backups=1 \"https://raw.githubusercontent.com/valegrajales/laboratorio4-sr-uniandes/main/data/book_descripcion_vectors_bert\" -O book_descripcion_vectors_bert\n",
        "!wget --no-cache --backups=1 \"https://raw.githubusercontent.com/valegrajales/laboratorio4-sr-uniandes/main/data/book_descripcion_vectors_mpnet\" -O book_descripcion_vectors_mpnet\n",
        "!wget --no-cache --backups=1 \"https://raw.githubusercontent.com/valegrajales/laboratorio4-sr-uniandes/main/data/book_descripcion_vectors_minilm\" -O book_descripcion_vectors_minilm"
      ],
      "metadata": {
        "id": "DZvN2B-ApBjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cargar datos de libros"
      ],
      "metadata": {
        "id": "LXgHSTqXZKtt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9UhRSjKVjdE"
      },
      "outputs": [],
      "source": [
        "df_books = pd.read_parquet(\"book_data.parquet.gzip\")\n",
        "df_books.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pregunta 1**\n",
        "\n",
        "Realice el análisis exploratorio de datos, aplicando las mismas técnicas vistas en el laboratorio 2"
      ],
      "metadata": {
        "id": "MigtPouOgWZB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BpPSgdLwgMMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar columnas, id de libro, ttulo y descripción, con la columna descripción se generan los vectores (embedding) con información semántica\n",
        "df_books = df_books[['book_id','title', 'description']]"
      ],
      "metadata": {
        "id": "HDY4BWHUKjuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_books.shape"
      ],
      "metadata": {
        "id": "vO7olGa9K1EL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de interacciones de usuarios con libros"
      ],
      "metadata": {
        "id": "1lW9ngpw0KsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_books_interactions = pd.read_csv('goodreads_interactions.csv')\n",
        "df_books_interactions.head()"
      ],
      "metadata": {
        "id": "MzmNR5LmXbu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_books_interactions.shape"
      ],
      "metadata": {
        "id": "eM59FoBfMYdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar el conjunto que queremos usar para evaluar el desempeño de los modelos\n",
        "df_books_interactions_train, df_books_interactions_test = train_test_split(df_books_interactions, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "3ZtEeQOIOM4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de vectores (embedding): BERT-large y MPNet\n",
        "\n",
        "En esta sección se trabajará con modelos pre-entrenados de modelos de lenguage BERT-large y MPNet que convierten texto a embeddings. \n",
        "\n",
        "Bidirectional Encoder Representations from Transformers (BERT) es una técnica de NLP (Natural Language Processing) desarrollada por Google y publicada en 2018 por Jacob Devlin. \n",
        "\n",
        "Actualmente Google utiliza BERT para entender las consultas de los usuarios en su buscador. \n",
        "\n",
        "Tiene dos versiones: \n",
        "- **BERT:** 12 capas, 12 cabezales de atencion y 110 millones de parámetros. Genera vectores de 768 dimensiones \n",
        "- **BERT-large:** 24 capas, 16 cabezales de atencion y 340 millones de parámetros.  \n",
        "\n",
        "![BERT y BERT-large](http://jalammar.github.io/images/bert-base-bert-large.png)\n",
        "\n",
        "![BERT y BERT-large arquitectura](http://jalammar.github.io/images/bert-base-bert-large-encoders.png)\n",
        "\n",
        "MPNet: Masked and Permuted Pre-training for Language Understanding, de Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu, es un novedoso método de preentrenamiento para tareas de comprensión del lenguaje. Resuelve los problemas de MLM (modelado del lenguaje enmascarado) en BERT y PLM (modelado del lenguaje permutado) en XLNet y consigue una mayor precisión. [MPNet](https://arxiv.org/pdf/1905.02450.pdf)\n",
        "\n",
        "En este caso los textos que utilizaremos son las descripciones de los libros y compararemos los resultados de recomendación con BERT-large y MPNet. El primer paso es generar los vectores de características (embedding) para luego esta representación dimensional semántica en la búsqueda de contenido.\n",
        "\n",
        "Para mayores detalles sobre el modelo de lenguaje BERT se recomienda revisar el siguiente artículo:\n",
        "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)"
      ],
      "metadata": {
        "id": "9KWcy0cWT9ZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de vectores usando modelo Bert"
      ],
      "metadata": {
        "id": "Ya5A4iJUVpzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reducir cantidad de datos a vectorizar, vamos a usar una muestra de 2000 libros, no modifique la semilla, ya que la muestra de interacciones a evaluar depende de la misma y los vectores se generaron con base en esta semilla\n",
        "df_books_small = df_books.sample(n=10000, random_state=42).fillna('')\n",
        "df_books_small.shape"
      ],
      "metadata": {
        "id": "xG2RCpPPdTTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diccionario indice a libro id y viceversa para hacer las recomendaciones las recomendaciones \n",
        "idx2bookid = {i: id_ for i, id_ in enumerate(df_books_small.book_id)}\n",
        "bookid2idx = {id_:i for i, id_ in enumerate(df_books_small.book_id)}"
      ],
      "metadata": {
        "id": "NY0Lmk5Zdh6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los vectores del modelo Bert\n",
        "with open('book_descripcion_vectors_bert', \"rb\") as fIn:\n",
        "  cache_data = pickle.load(fIn)\n",
        "  book_descripcion_vectors_bert = cache_data['embeddings']"
      ],
      "metadata": {
        "id": "jZdXVSowSFmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los vectores del modelo MPNet\n",
        "with open('book_descripcion_vectors_mpnet', \"rb\") as fIn:\n",
        "  cache_data = pickle.load(fIn)\n",
        "  book_descripcion_vectors_mpnet = cache_data['embeddings']"
      ],
      "metadata": {
        "id": "RLpQxx-ZTTDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los vectores del modelo MiniLM\n",
        "with open('book_descripcion_vectors_minilm', \"rb\") as fIn:\n",
        "  cache_data = pickle.load(fIn)\n",
        "  book_descripcion_vectors_minilm = cache_data['embeddings']"
      ],
      "metadata": {
        "id": "G7LXfXmsTv0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pregunta 2** \n",
        "\n",
        "Considerando que haremos un recomendador basado en contenidos ¿Por qué el uso de modelos de lenguage es una buena elección para este tipo de problema?"
      ],
      "metadata": {
        "id": "yzs_ncbDaTHs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1XGvm-jSpxUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probamos con BERT, MPNet y MiniLM reduciendo dimensionalidad con PCA-20\n",
        "\n",
        "Una vez calculados (o cargados) los vectores característicos de cada libro a partir de su descripción, reducimos dimensionalidad. Probaremos con BERT, MPNet y MiniLM para comparar los resultados en recomendación basada en contenido. "
      ],
      "metadata": {
        "id": "6OC2-L4DUkCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Project into a 20 PCA feature space\n",
        "pca20_bert_featvectors = PCA(n_components=20).fit_transform(book_descripcion_vectors_bert)\n",
        "pca20_mpnet_featvectors = PCA(n_components=20).fit_transform(book_descripcion_vectors_mpnet)\n",
        "pca20_minilm_featvectors = PCA(n_components=20).fit_transform(book_descripcion_vectors_minilm)"
      ],
      "metadata": {
        "id": "DTNFNxwcUi51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca20_bert_featvectors.shape"
      ],
      "metadata": {
        "id": "QLZYIWi7W8O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca20_mpnet_featvectors.shape"
      ],
      "metadata": {
        "id": "qkRqm-h6W-eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca20_minilm_featvectors.shape"
      ],
      "metadata": {
        "id": "8qauh6fHXA5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pregunta 3**\n",
        "\n",
        "Comente por qué se utiliza PCA (investigue) para reducir la dimensión de cada vector característico. ¿Qué sucede con la pérdida de información en la reducción de dimensionalidad?"
      ],
      "metadata": {
        "id": "TY1yYaw1XV_0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pymNQFMvXDrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recuperación de documentos similares \n",
        "\n",
        "En esta sección utilizaremos los vectores cargados para hacer un sistema de recuperación o búsqueda de información, para diferentes métricas de distancia.\n",
        "\n",
        "Buscamos libros similares de acuerdo a la representación vectorial (BERT) de su descripción."
      ],
      "metadata": {
        "id": "att8AlRXl7lN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# formato de resultados \n",
        "pd.options.display.max_colwidth = 50\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "metadata": {
        "id": "jMkee7rGl6yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find similar books by book id\n",
        "def find_similar_books(embedding, query_id=None, metric='euclidean', topk=10):\n",
        "    \n",
        "    n = embedding.shape[0]\n",
        "    \n",
        "    if query_id is None:\n",
        "        query_i = np.random.randint(n)\n",
        "        query_id = idx2bookid[query_i]\n",
        "    \n",
        "    else:\n",
        "        query_i = bookid2idx[query_id]\n",
        "        \n",
        "    \n",
        "    distances = pairwise_distances(embedding[query_i].reshape(1,-1), embedding, metric=metric)\n",
        "    heap = []\n",
        "    for i in range(n):            \n",
        "        if len(heap) < topk:\n",
        "            heapq.heappush(heap, (-distances[0][i], i))\n",
        "        else:\n",
        "            heapq.heappushpop(heap, (-distances[0][i], i))\n",
        "\n",
        "    heap.sort(reverse=True)\n",
        "    rec_ids = [idx2bookid[i] for _,i in heap]\n",
        "    \n",
        "    return rec_ids"
      ],
      "metadata": {
        "id": "mCUvCVgamhHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando BERT"
      ],
      "metadata": {
        "id": "dSrO1pufnoKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# libros similares al libro de id 27421523 (Harry Potter and the Sorcerer's Stone) utilizando distancia euclideana. se puede cambiar a \"cosine\" \n",
        "similar_books = find_similar_books(book_descripcion_vectors_bert, query_id = '27421523', metric = 'euclidean', topk=10 )\n",
        "similar_books"
      ],
      "metadata": {
        "id": "r7h7eGl4nfCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_books_small[df_books_small.book_id.isin(similar_books)][['book_id', 'title', 'description']]"
      ],
      "metadata": {
        "id": "c20exhtHokfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando BERT reducidos con PCA "
      ],
      "metadata": {
        "id": "sV6XsEOJqkVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# libros similares al libro de id 27421523 (Harry Potter and the Sorcerer's Stone) utilizando distancia euclideana. se puede cambiar a \"cosine\" \n",
        "similar_books = find_similar_books(pca20_bert_featvectors, query_id = '27421523', metric = 'euclidean', topk=10 )\n",
        "similar_books"
      ],
      "metadata": {
        "id": "tbr9KCnCpXjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_books_small[df_books_small.book_id.isin(similar_books)][['book_id', 'title', 'description']]"
      ],
      "metadata": {
        "id": "GORIbbs3qrxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando MPNet"
      ],
      "metadata": {
        "id": "PSGUYZ2krVDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# libros similares al libro de id 27421523 (Harry Potter and the Sorcerer's Stone) utilizando distancia euclideana. se puede cambiar a \"cosine\" \n",
        "similar_books = find_similar_books(book_descripcion_vectors_mpnet, query_id = '27421523', metric = 'euclidean', topk=10 )\n",
        "similar_books"
      ],
      "metadata": {
        "id": "lPD_PGqAq6Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_books_small[df_books_small.book_id.isin(similar_books)][['book_id', 'title', 'description']]"
      ],
      "metadata": {
        "id": "MUTSt5vwre9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando MPNet reducido con PCA"
      ],
      "metadata": {
        "id": "5Kh5FvUGry25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# libros similares al libro de id 27421523 (Harry Potter and the Sorcerer's Stone) utilizando distancia euclideana. se puede cambiar a \"cosine\" \n",
        "similar_books = find_similar_books(pca20_mpnet_featvectors, query_id = '27421523', metric = 'euclidean', topk=10 )\n",
        "similar_books"
      ],
      "metadata": {
        "id": "QJoOkYZoriBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_books_small[df_books_small.book_id.isin(similar_books)][['book_id', 'title', 'description']]"
      ],
      "metadata": {
        "id": "DYV-d_8wsJwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando MiniLM"
      ],
      "metadata": {
        "id": "oGM7D1DEsSjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# libros similares al libro de id 27421523 (Harry Potter and the Sorcerer's Stone) utilizando distancia euclideana. se puede cambiar a \"cosine\" \n",
        "similar_books = find_similar_books(book_descripcion_vectors_minilm, query_id = '27421523', metric = 'euclidean', topk=10 )\n",
        "similar_books"
      ],
      "metadata": {
        "id": "65HAydl8sWif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_books_small[df_books_small.book_id.isin(similar_books)][['book_id', 'title', 'description']]"
      ],
      "metadata": {
        "id": "T56HTwbFsnRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando MiniLM reducido con PCA"
      ],
      "metadata": {
        "id": "ROagGRFesXcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# libros similares al libro de id 27421523 (Harry Potter and the Sorcerer's Stone) utilizando distancia euclideana. se puede cambiar a \"cosine\" \n",
        "similar_books = find_similar_books(pca20_minilm_featvectors, query_id = '27421523', metric = 'euclidean', topk=10 )\n",
        "similar_books"
      ],
      "metadata": {
        "id": "N5kz8r-VsNOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_books_small[df_books_small.book_id.isin(similar_books)][['book_id', 'title', 'description']]"
      ],
      "metadata": {
        "id": "KCHBWnRws-0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pregunta 4: \n",
        "Comente los resultados obtenidos, en cuanto a modelo de lenguaje, reduccion de dimensionalidad y métrica de distancia utilizada."
      ],
      "metadata": {
        "id": "169FM0LbtMvi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9-K_cx79tCFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recomendaciones "
      ],
      "metadata": {
        "id": "1vLGzgC3tb3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# formato de resultados \n",
        "pd.options.display.max_colwidth = 50\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "metadata": {
        "id": "4UaXPsW3tdoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend(embedding, user_id=None, topk=10, metric='cosine'):\n",
        "    \n",
        "    #print(\"user_id = \", user_id)\n",
        "    \n",
        "    #user_id = str(user_id)\n",
        "    \n",
        "    #Calculate distance metrics\n",
        "    trx = df_books_interactions.loc[df_books_interactions['user_id'] == user_id].book_id.tolist()\n",
        "    #trx = df_books_interactions[user_id]\n",
        "    n = embedding.shape[0]\n",
        "    distances = 1e9\n",
        "    \n",
        "    # recorremos transacciones pasadas del usuario \n",
        "    for t in trx:\n",
        "        query_i = bookid2idx[str(t)]\n",
        "        \n",
        "        # recomendamos items más cercanos a items con los que interactuó el usuario\n",
        "        distances = np.minimum(distances, pairwise_distances(\n",
        "                embedding[query_i].reshape(1,-1), embedding, metric=metric).reshape(-1))\n",
        "\n",
        "    #Rank items de menor a mayor distancia (nos quedamos con los topk)\n",
        "    trx_set = set(trx)\n",
        "    heap = []\n",
        "    for i in range(n):\n",
        "        if idx2bookid[i] in trx_set:\n",
        "            continue\n",
        "        if len(heap) < topk:\n",
        "            heapq.heappush(heap, (-distances[i], i))\n",
        "        else:\n",
        "            heapq.heappushpop(heap, (-distances[i], i))\n",
        "    heap.sort(reverse=True)\n",
        "    \n",
        "    # utilizamos un heap para extraer los items ordenados de menor a mayor distancia \n",
        "    recommended_ids = [idx2bookid[i] for _,i in heap]\n",
        "    \n",
        "    # retornar los que el usuario no haya consumido \n",
        "    filtered_recommended_ids = []\n",
        "    \n",
        "    return recommended_ids"
      ],
      "metadata": {
        "id": "5lcfpvL0tk1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generar recomendaciones para un usuario específico"
      ],
      "metadata": {
        "id": "Nl2luJZAuJTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# recomendación para el usuario id = 126522, utilizando MPNet\n",
        "user_id = 126522\n",
        "rec = recommend(book_descripcion_vectors_mpnet, user_id=user_id, topk=15)\n",
        "rec "
      ],
      "metadata": {
        "id": "iTjBFUNPt6n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## transacciones pasadas del usuario "
      ],
      "metadata": {
        "id": "OPcVmWQXLWPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "past_interactions = df_books_interactions.loc[df_books_interactions['user_id'] == user_id]\n",
        "df_books_small[df_books_small.book_id.isin(past_interactions.book_id.astype(str))][['book_id', 'title', 'description']]"
      ],
      "metadata": {
        "id": "a9zT_RCCLW9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## información de recomendaciones"
      ],
      "metadata": {
        "id": "LRuEfybwNQe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_books_small[df_books_small.book_id.isin(rec)][['book_id', 'title', 'description']]"
      ],
      "metadata": {
        "id": "A5DQ2ocEMID2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluación de las recomendaciones con interacciones de testing "
      ],
      "metadata": {
        "id": "NphXpeVuOv-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Métricas de evaluación \n",
        "# Obtenido de https://gist.github.com/bwhite/3726239\n",
        "\n",
        "def precision_at_k(r, k):\n",
        "    assert k >= 1\n",
        "    r = np.asarray(r)[:k] != 0\n",
        "    if r.size != k:\n",
        "        raise ValueError('Relevance score length < k')\n",
        "    return np.mean(r)\n",
        "\n",
        "def average_precision(r):\n",
        "    r = np.asarray(r) != 0\n",
        "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
        "    if not out:\n",
        "        return 0.\n",
        "    return np.mean(out)\n",
        "\n",
        "def mean_average_precision(rs):\n",
        "    return np.mean([average_precision(r) for r in rs])\n",
        "  \n",
        "def dcg_at_k(r, k):\n",
        "    r = np.asfarray(r)[:k]\n",
        "    if r.size:\n",
        "        return np.sum(np.subtract(np.power(2, r), 1) / np.log2(np.arange(2, r.size + 2)))\n",
        "    return 0.\n",
        "\n",
        "\n",
        "def ndcg_at_k(r, k):\n",
        "    idcg = dcg_at_k(sorted(r, reverse=True), k)\n",
        "\n",
        "    if not idcg:\n",
        "        return 0.\n",
        "    return dcg_at_k(r, k) / idcg"
      ],
      "metadata": {
        "id": "6Y--UBLeNDXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación de recomendación con MPNet"
      ],
      "metadata": {
        "id": "Qec7hN5AO9TF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "\n",
        "mean_map = 0.\n",
        "mean_ndcg = 0.\n",
        "\n",
        "embeddings = book_descripcion_vectors_mpnet\n",
        "topk = 10 \n",
        "\n",
        "for i, u in enumerate(df_books_interactions_test.user_id.tolist()):\n",
        "    \n",
        "    print(i, end= '\\r')\n",
        "    \n",
        "    rec = recommend(embeddings, user_id = u, topk=topk)\n",
        "    rel_vector = [np.isin(df_books_interactions.loc[df_books_interactions['user_id'] == u].book_id.tolist(), [eval(i) for i in rec], assume_unique=True).astype(int)]\n",
        "    mean_map += mean_average_precision(rel_vector)\n",
        "    mean_ndcg += ndcg_at_k(rel_vector, topk)\n",
        "\n",
        "mean_map /= len(df_books_interactions_test)\n",
        "mean_ndcg /= len(df_books_interactions_test)\n",
        "\n",
        "time_taken = time.time() - start"
      ],
      "metadata": {
        "id": "rHQvGl1MO5ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('MAP ',mean_map)\n",
        "print('ndcg@10' ,mean_ndcg)\n",
        "print('tiempo de ejecucion {0:.2f} segs'.format(time_taken))"
      ],
      "metadata": {
        "id": "FDWvwNXbRgs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación de recomendación con MPNet reducidos con PCA-20"
      ],
      "metadata": {
        "id": "skp80lqNTsYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "\n",
        "mean_map = 0.\n",
        "mean_ndcg = 0.\n",
        "\n",
        "embeddings = pca20_mpnet_featvectors\n",
        "topk = 10 \n",
        "\n",
        "for i, u in enumerate(df_books_interactions_test.user_id.tolist()):\n",
        "    \n",
        "    print(i, end= '\\r')\n",
        "    \n",
        "    rec = recommend(embeddings, user_id = u, topk=topk)\n",
        "    rel_vector = [np.isin(df_books_interactions.loc[df_books_interactions['user_id'] == u].book_id.tolist(), [eval(i) for i in rec], assume_unique=True).astype(int)]\n",
        "    mean_map += mean_average_precision(rel_vector)\n",
        "    mean_ndcg += ndcg_at_k(rel_vector, topk)\n",
        "\n",
        "mean_map /= len(df_books_interactions_test)\n",
        "mean_ndcg /= len(df_books_interactions_test)\n",
        "\n",
        "time_taken = time.time() - start"
      ],
      "metadata": {
        "id": "97KxnOirR__t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('MAP ',mean_map)\n",
        "print('ndcg@10' ,mean_ndcg)\n",
        "print('tiempo de ejecucion {0:.2f} segs'.format(time_taken))"
      ],
      "metadata": {
        "id": "dsf9ruvyT4bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pregunta 5: \n",
        "Comente los resultados en términos de tiempo de ejecución y métricas de ranking para los 2 modelos."
      ],
      "metadata": {
        "id": "RjX97M-8Y1sD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6RP2LFy7T-i6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}